# Place for all Transformer based Language Models

## Indonesian GPT2-small
This is my first attempt to create a small indonesian GPT2 Language Model. It was trained only with indonesian 
Wikipedia as dataset for 5 hours and it reaches a perplexity of around 27. It is hosted at 
[huggingface](https://huggingface.co/cahya/gpt2-indonesian-small). A documentation and a better GPT2 Language Model 
trained with more indonesian datasets will follow. If you have dataset or just any suggestion which datasets 
I could use to train, just let me know please. Thanks.
